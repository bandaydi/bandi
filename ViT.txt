@ BERT

- Bidirectional Encoder Representations from Transformers
> Language Representation을 해결하기 위해 고안된 구조
>> 즉, 단어, 문장, 언어를 어떻게 표현할까에 관한 고민
>> 이를 잘 표현할 수 있다면 다양한 task를 해결할 수 있다.

- 기존 단방향성 모델은 성능 향상, 문맥 파악에 한계점이 존재했고, 이를 해결하기 위해 양방향성을 띄는 모델을 제안하는 방향으로 진행된 것

- Bert는 pre-training이 가능한 모델
> 이전에 존재하던 NLP 모델은 pre-training이 어려웠기 때문에 특정 task가 존재할 경우 처음부터 학습시켜야 하는 단점이 존재
> 각종 Image task를 해결하는 데 있어서도 pre-training이 매우 큰 역할을 하고 있다는 점에서 NLP에서도 여러 task에 transfer하기 위해서는 이를 활용할 수 있는 모델이 매우 시급했었다.

- 엄청난 수의 Wikipedia와 BooksCorpus 단어를 학습한 BERT를 다양한 task에 적용하고, fine-tuning해서 사용할 수 있는 것이 매우 큰 장점

- NLP task
> Question and Answering
> Machine Traslation
> 문장 주제 찾기 또는 분류하기
> 사람처럼 대화하기

- transformer 구조를 중점적으로 사용한 구조, 특히 self-attention layer를 여러 개 사용하여 문장에 포함되어 있는 token 사이의 의미 관계를 잘 추출할 수 있다.

- BERT는 transformer의 encoder부분만 사용하여 학습을 진행
> 두 가지 대표적인 학습 방법으로 encoder를 학습시킨 후에 특정 task의 fine-tuning을 활용하여 결과물을 얻는 방법으로 사용

- Pre-training
> 두 가지 unsupervised 방법
> Masked Language Model(MLM)
>> 문장에서 단어 중의 일부를 [Mask] 토큰으로 바꾼 뒤, 가려진 단어를 예측하도록 학습, 이 과정에서 문맥을 파악하는 능력을 기름
>> 추가적으로 더욱 다양한 표현을 학습할 수 있도록 80%는 [Mask] 토큰으로 바꾸어 학습하지만, 나머지 10%는 token을 random word로 바꾸고, 마지막 10%는 원본 word 그대로를 사용
> Next Sentance Model(NSP)
>> 다음 문장이 올바른 문장인지 맞추는 문제, 두 문장 사이의 관계를 학습
>> 문장 A와 B를 이어 붙이는데, B는 50% 확률로 관련 있는 문장(IsNext label) 또는 관련 없는 문장(NotNext label)을 사용
>> QA나 NLI task의 성능 향상에 영향을 끼침

- 학습된 BERT를 fine-tuning할 때는 (Classification task 라면)Image task에서의 fine-tuning과 비슷하게 class label 개수 만큼의 output을 가지는 Dense Layer를 붙여서 사용

- input Representation
> BERT는 학습을 위해 기존 transformer의 input 구조를 사용하면서도 추가로 변형하여 사용, Tokenization은 WordPiece 방법을 사용
> 세 가지 임베딩(Token, Segment, Position)을 사용해서 문장을 표현
> 최종적으로 세 가지 임베딩을 더한 임베딩을 input으로 사용

- 기존에 사용되던 워드 임베딩 방식의 임베딩을 사용하지 않고 Word Piece 임베딩 방식을 사용
> 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 만드는 방식
>> 즉, 자주 등장하는 sub-word는 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 sub-word로 쪼게지게 된다.
>> 기존 워드 임베딩 방법은 Out-of-vocabulary(OOV)문제가 있음. 즉, 희긔 단어, 이름, 숫자 나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있었다.
>> 그러나 Word Piece 임베딩은 모든 언어에 적용 가능하며 sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도도 상승하는 효과가 있다.

- Token Embedding
> 두 가지 특수 토큰(CLS, SEP)을 사용하여 문장을 구별
> Special Classification token(CLS)은 모든 문장의 가장 첫 번째(문장의 시작) 토큰으로 삽입, 이 토큰은 classification task에서만 사용, 이외엔 무시
> Special Seperator token(SEP)을 사용하여 각 문장을 구별, 여기에 Segment Embedding을 더해서 앞뒤 문장을 더욱 쉽게 수별할 수 있도록 도와줌
>> 이 토큰은 각 문장의 끝에 삽입

- BERT에 쓰이는 Transfer Learning(TL) 기법 2가지
> BERT를 써먹는다는 것은 전이 학습을 이용한다는 것
>> 이미 엄청나게 많은 텍스트 데이터를 통해 학습하여 BERT라는 모델에 축약된 지식을 내가 가진 적은 양의 데이터에다가 적용한다는 것

- 대표적으로 사용되는 두 가지 TL 기법
> Feature Extraction
>> BERT를 블랙박스 함수라 했을 때 BERT가 내뱉는 vector(output)을 다른 머신러닝 모델의 input으로 쓰는 것(마치 Bag-of-Word Vector를 input으로 쓰는것처럼)
>> BERT의 파라미터(블랙 박스 내부의)는 고정되어 변하지 않고, 새로 얹은 새 모델만 가진 데이터를 가지고 학습이 됨
> Fine-tuning
>> 데이터가 조금 더 많이 있을 때 쓰면 좋은 방식
>> BERT의 파라미터 역시 주어진 데이터를 통해 재학습 시키는 것
>> 아무런 머신러닝 모델을 붙일 수 있는게 아니라 Sigmoid function이나 multi-layer perceptron(MLP)처럼 Back-propagation이 가능해야 한다.
>> 안그러면 같은 cost function으로 기존 BERT 모델이 재학습되지 않기 때문

- BERT의 약점
> 다른 LM처럼 텍스트를 생성(Text Generation) 하기 힘들다.
>> 다음 단어가 아닌 중간에 있는 단어를 예측하는 방식으로 학습되었기 때문
>> 2020년에 이 방면의 괴물로 나타난 모델이 바로 GPT, BERT의 강력한 사촌

- NLP(자연어 처리) 사전 훈련 기술, 범용 Language Model

- BERT 등장 이전에는 '데이터의 전처리 임베딩'을 Word2Vec, Glove, Fasttext 방식을 많이 사용
> 임베딩 과정에서 BERT를 사용하는 것이고 BERT는 특정 모델(분류, 인식 등)을 시작 하기 전 사전 훈련 Embedding을 통해 특정 모델의 성능을 더 좋게 할 수 있는 언어 모델

- 총 33억개 단어(BookCorpus 8억개 단어 + Wikipedia 25 억개 단어)의 거대한 말뭉치를 이용하여 학습
> 거대한 말뭉치를 MLM, NSP 모델 적용을 위해 스스로 라벨을 만들고 수행하여 준지도 학습이라고 한다.

-BERT는 2가지 버전이 있슴
> BERT-base, BERT-large
>> Trasformer 블록의 숫자, hidden size, Transformer의 Attention block 숫자의 차이 이며
>> base는 1.1억개의 학습 파라미터, large는 3.4억개의 학습 파라미터가 존재
>> 학습 파라미터가 매우 많기 때문에 학습시간이 무척 오래 걸림

- 전이학습은 label이 주어지므로 지도 학습 supervised learning이다.

- 전이 학습은 BERT의 언어 모델의 출력에 추가적인 모델을 쌓아 만듭니다. 일반적으로 복잡한 CNN, LSTM, Attention을 쌓지 않고 간단한 DNN만 쌓아도 성능이 잘 나오며 별 차이가 없다고 알려져 있음

- 토큰 유형
> CLS : task에 대한 정보 토큰
> SEP : 문장 1, 2의 끝을 알려주는 토큰
> MASK : 예측하는 target 토큰

- 임베딩 vector 종류
> Input embedding
> Positional embedding
> Segment embedding : 문장 1, 2에 대한 구분 정보

- LayerNorm & Dropout

- Key(to) Masking(받는 마스킹)만 사용

- Multi-head attention을 concat 후 Dense layer을 적용한 후 Dropout

- 활성화함수는 GeLU

- output Dropout

- BERT의 서브워드 토크나이저 : WordPiece
> BERT는 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저를 사용합니다. 글자로부터 서브워드들을 병합해가는 방식으로 최종 단어 집합(Vocabulary)을 만든다.
> 서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어의 경우에는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어를 갖고있습니다. 이렇게 단어 집합이 만들어지고 나면, 이 단어 집합을 기반으로 토큰화를 수행합니다.

- Masked Language Model
> BERT는 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 15%의 단어를 랜덤으로 마스킹(Masking)합니다. 그리고 인공 신경망에게 이 가려진 단어들을(Masked words) 예측하도록 합니다.
> 더 정확히는 전부 [MASK]로 변경하지는 않고, 랜덤으로 선택된 15%의 단어들은 다시 다음과 같은 비율로 규칙이 적용됩니다.
> 10%의 단어들은 랜덤으로 단어가 변경된다.
> 10%의 단어들은 동일하게 둔다.
> 이렇게 하는 이유는 [MASK]만 사용할 경우에는 [MASK] 토큰이 파인 튜닝 단계에서는 나타나지 않으므로 사전 학습 단계와 파인 튜닝 단계에서의 불일치가 발생하는 문제가 있습니다. 이 문제을 완화하기 위해서 랜덤으로 선택된 15%의 단어들의 모든 토큰을 [MASK]로 사용하지 않습니다.
> 전체 단어의 85%는 마스크드 언어 모델의 학습에 사용되지 않습니다. 마스크드 언어 모델의 학습에 사용되는 단어는 전체 단어의 15%입니다. 학습에 사용되는 12%는 [MASK]로 변경 후에 원래 단어를 예측합니다. 1.5%는 랜덤으로 단어가 변경된 후에 원래 단어를 예측합니다. 1.5%는 단어가 변경되지는 않았지만, BERT는 이 단어가 변경된 단어인지 원래 단어가 맞는지는 알 수 없습니다. 이 경우에도 BERT는 원래 단어가 무엇인지를 예측하도록 합니다.

- Next Sentence Prediction
> BERT는 두 개의 문장을 준 후에 이 문장이 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련시킵니다. 이를 위해서 50:50 비율로 실제 이어지는 두 개의 문장과 랜덤으로 이어붙인 두 개의 문장을 주고 훈련시킵니다. 
> BERT의 입력으로 넣을 때에는 [SEP]라는 특별 토큰을 사용해서 문장을 구분합니다. 첫번째 문장의 끝에 [SEP] 토큰을 넣고, 두번째 문장이 끝나면 역시 [SEP] 토큰을 붙여줍니다. 그리고 이 두 문장이 실제 이어지는 문장인지 아닌지를 [CLS] 토큰의 위치의 출력층에서 이진 분류 문제를 풀도록 합니다. [CLS] 토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰입니다. 그리고 위의 그림에서 나타난 것과 같이 마스크드 언어 모델과 다음 문장 예측은 따로 학습하는 것이 아닌 loss를 합하여 학습이 동시에 이루어집니다.
> 두 문장의 관계를 이해하는 것이 중요한 태스크들에서 중요한 역할

- BERT는 총 3개의 임베딩 층이 사용됩니다.
> WordPiece Embedding : 실질적인 입력이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기로 30,522개.
> Position Embedding : 위치 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.
> Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.

@ GPT

- decoder 부분을 사용하여 text generation 문제를 해결하는 모델

- 2018

@ ViT

- Image representation을 해결하기 위한 모델??

@ ETC

- BERT, GPT는 스케일이 대단하다? 데이터가 미친듯이 많다
> 최근 몇 년간 NLP 연구의 트렌드는 "더 많이, 더 크게"

- 남의 덕을 보고 살자는 철학을 가진(ㅋㅋ) 전이 학습 Transfer learning

- representation(표현) 이란
> 데이터를 유용한 feature로 맵핑하는 과정
> 데이터를 인코딩(encoding)하거나 묘사하기 위해 데이터를 바라보는 다른 방법, 머신 러닝 모델은 입력 데이터에서 적절한 표현을 찾는 것

- 자기지도학습(Self-supervised learning)
> 자기지도학습은 사람 라벨링 없이도 대량의 raw 데이터를 활용하여 모델이 인풋에 대한 좋은 representation을 생성하는 방법을 배우는 방법이다.
> 언어 AI의 경우  GPT, BERT 등의 모델은 자기지도학습을 통해 도메인과 무관한(domain agnostic) 특성을 학습하였고, 다운스트림 태스크에서 좋은 성능을 보였다.
> 특히 이 모델들은 임의 길이의 1차원의 시퀀스를 처리하기 위해 Transformer 아키텍쳐를 사용하였다.


# NLP

- 이 논문이 transformer model을 이용하여 generative pre-training과 discriminative fine-tuning을 통해 강력한 NLU(natural language understanding)을 할 수 있다는 것을 증명하며, 해당 방법의 신호탄을 열었다.

- 자연어 처리

- 사전 훈련(pre-training) 언어 모델

- 전이학습 Transfer learning
> 하나의 문제를 푸는 동안 지식을 저장하는데 초점을 두고, 다르지만 연관된 문제를 푸는데 그 지식을 활용하는 것

- 신경망은 데이터를 가지고 학습이 된다. 신경망이 데이터를 통해 학습을 한 지식은 네트워크의 '가중치(weight)'로 정리된다. 이 가중치들은 따로 추출하여 다른 신경망에 전송(transfer) 될 수 있따.
> 즉, 다른 신경망을 처음부터 학습하는 대신에, 학습된 특성들을 전송(transfer)하는 것.
> 모델은 데이터를 통해서 학습이 되기 때문에, 사전훈련 모델을 만들기 위해서는 방대한 데이터가 필요

- NLP에서는 가지고 있는 텍스트 데이터를 corpus라고 부른다.

# Transfer learning

- 첫 번쨰 핵심, 각 문제 간의 유사성

- 비슷한 일을 하던 사람은 금방 따라잡을 수 있다!

- Computer Vision 의 이미지 처리에서 활발히 쓰인 전이 학습

- ImageNet과 같이 엄청나게 많은 종류의 물체의 사진이 있는 데이터를 가지고 딥러닝 모델인 CNN을 학습, 여기서의 문제는 주어진 이미지의 종류를 분류하는 일
> 학습한 딥러닝 모델들이 사물 인식에 필요한 기초 개념들을 스스로 파악하고 내부 파라미터(parameter)들에 어떤 형식으로 저장하고 있다는 것을 파악했다.
> 딥러닝은 다양한 층의 파라미터로 구성이 되는데 어떤 층은 선과 꼭짓점에 대한 파악을 하고, 어떤 층은 색과 패턴에 대해서 배운다.
>> 이런 식으로 딥러닝 모델이 스스로 최종 문제(ex. 사물 종류 분류)를 위해 중간 단계의 개념들을 구조화 하는 것을 representation learning 이라고 한다.
> 이렇게 학습된 파라미터들이 다른 이미지 처리 문제들에 매우 유용하다는 사실을 알아냄

- 두 번째 핵심, 데이터 효율성

- 많고 유사한 데이터로 미리 학습된 모델을 Pretrained Model 이라고 한다.

- BERT와 같이 구글이나 페이스북 같은 대기업에서 엄청나게 많은 데이터와 컴퓨팅 파워로 학습된 모델을 공개

- crawling과 잘 어울리는 단어들에 대한 정보는 문장을 읽기만 하면 알 수 있다.
> crawling이 등장하는 수백, 수천, 아니 수만 개의 문장을 찾아서 보면 된다.
> machine learning 에서는 이런 종류의 학습 방법을 unsupervised learning이라고 한다.
> 따로 시간과 돈을 들여 데이터를 가공할 필요 없이 그냥 책이나 인터넷에 있는 텍스트 그대로만 가지고도 유용한 모델을 만들 수 있따는 것

- Transfer Learning 이란?
> 기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법입니다.

- 왜 사용하나?
> 복잡한 모델일수록 학습시키기 어렵습니다. 어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다.
> layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다.
> 결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다.

@ NLP에서의 TL?

- NLP에서 활용할 수 있는 Pretrained Model은 대표적으로 Word Embedding과 Language Model 이다.

- word embedding
> word embedding들을 다른 NLP task의 input으로 사용하였을 때, 엄청난 성능 증가를 기대할 수 있다. 
>> word embedding은 엄청난 양의 corpus로 학습되어 각 단어에 대한 정보를 꽤나 정확하고 깊게 담아낼 수 있기 때문
> 각 주변 단어들을 이용해서 한 단어의 문법적, 의미적 정보를 하나의 Vector에 담아낸다.

- Language Model
> Sentence Embedding을 계산할 수 있다.
> 단어 한 개를 1개의 vector로 표현한 것이 word embedding 인 것처럼
>> LM이 많은 데이터를 보면서 학습한 모델이 새로운 문장을 보았을 때, 그 이해를 압축시켜서 하나의 vector에 담아내는 것이라고 이해할 수 있따.
> LM, 언어 모델은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당(assign)하는 모델
> 즉, 가장 자연스러운 단어 시퀸스를 찾아내는 모델
>> 단어 시퀀스에 확률을 할당하게 하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것
>> 다른 유형의 언어 모델로는 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델이 있다. like a 빈칸 추론 문제
> 언어 모델에 -ing를 붙인 언어 모델링 Language Modeling은 주어진 단어들로 부터 아직 모르는 단어를 예측하는 작업을 말한다.
>> 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링
> 크게 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 나뉨